---
title: "Informe Proyecto 2 entrega 7"
author:
- "Juan Luis Solórzano (carnet: 201598)"
- "Micaela Yataz (carnet: 18960)"
date: "2025-01-20"
output: pdf_document
---

<https://github.com/JusSolo/Mineria_Proyecto2.git>

```{r, include=FALSE , librerias}
# Cargar librerías necesarias
library(randomForest)
library(dplyr)
library(GGally)
library(ggplot2)
library(tidyr)
library(caret)
library(rpart)
library(rpart.plot)
library(glmnet)
library(e1071)
library(Metrics)
library(tidyverse)
library(class)
library(corrplot)
library(MLmetrics)
library(kernlab)


# Cargar la base de datos
datos <- read.csv('train.csv')

# Definir variables cuantitativas
vars_cuantitativas <- c("SalePrice", "LotFrontage", "LotArea", "OverallQual", "OverallCond", 
                        "YearBuilt", "YearRemodAdd", "MasVnrArea", "BsmtFinSF1", "BsmtFinSF2", 
                        "BsmtUnfSF", "TotalBsmtSF", "X1stFlrSF", "X2ndFlrSF", "LowQualFinSF", 
                        "GrLivArea", "BsmtFullBath", "BsmtHalfBath", "FullBath", "HalfBath", 
                        "BedroomAbvGr", "KitchenAbvGr", "TotRmsAbvGrd", "Fireplaces", "GarageYrBlt", 
                        "GarageCars", "GarageArea", "WoodDeckSF", "OpenPorchSF", "EnclosedPorch", 
                        "X3SsnPorch", "ScreenPorch", "PoolArea", "MiscVal", "MoSold", "YrSold")

# Imputar valores faltantes en variables específicas
datos <- datos %>%
  mutate(
    LotFrontage = ifelse(is.na(LotFrontage), 0, LotFrontage),
    MasVnrArea = ifelse(is.na(MasVnrArea), 0, MasVnrArea),
    GarageYrBlt = ifelse(is.na(GarageYrBlt), median(GarageYrBlt, na.rm = TRUE), GarageYrBlt)
  )

# Seleccionar solo variables cuantitativas
datosC <- datos[, vars_cuantitativas]
```

```{r, recuperacion de el training y test, include=FALSE}
y <- datos$SalePrice
set.seed(123)
trainI<- createDataPartition(y, p=0.7, list=FALSE)

train <- datosC[trainI, ]
test <- datosC[-trainI, ]


train$precio_categoria<-cut(train$SalePrice, 
                            breaks = c(0, 129975, 214000,Inf),
                            labels = c("Economica", "Intermedia", "Cara" ),
                            include.lowest = TRUE)

test$precio_categoria<-cut(test$SalePrice, 
                            breaks = c(0, 129975, 214000, Inf),
                            labels = c("Economica", "Intermedia", "Cara"),
                            include.lowest = TRUE)





```

```{r, modelos de entregas anteriores, include=FALSE}


# ---------------------------------------------------------------
# Modelos de REGRESIÓN
# ---------------------------------------------------------------

# 1. Regresión Lineal (Entregas 1 y 3)
Rlineal <- train(
  SalePrice ~ . -precio_categoria,
  data = train,
  method = "glmnet",
  tuneGrid = expand.grid(alpha = 1, lambda = 0.001),  # Mejor ajuste de entregas anteriores
  trControl = trainControl(method = "cv", number = 5)
)

# 2. Árbol de Regresión (Entrega 2)
Rarbol <- rpart(
  SalePrice ~ .-precio_categoria,
  data = train,
  control = rpart.control(maxdepth = 5, cp = 0.01)  # Parámetros óptimos
)

# 3. Naive Bayes para Regresión (Entrega 3)
RnaiveBayes <- naiveBayes(
  SalePrice ~ .-precio_categoria,
  data = train
)

# 4. KNN para Regresión (Entrega 4)
set.seed(123)
RKNN <- train(
  SalePrice ~ .-precio_categoria,
  data = train,
  method = "knn",
  tuneGrid = expand.grid(k = 9),  # Mejor k según entrega 4
  preProcess = c("center", "scale")
)

# 5. Regresión Logística (Entregas 5-6) - Ajustado para regresión
RLogistic <- glm(
  SalePrice ~ .-precio_categoria,
  data = train,
  family = gaussian()
)

# ---------------------------------------------------------------
# Modelos de CLASIFICACIÓN
# ---------------------------------------------------------------

# 1. Regresión Logística para Clasificación (Entrega 5)
CLogistic <- train(
  precio_categoria ~ .-SalePrice,
  data = train,
  method = "multinom",
  trControl = trainControl(method = "cv", number = 5)
)

# 2. Árbol de Clasificación (Entrega 2)
Carbol <- rpart(
  precio_categoria ~ .-SalePrice,
  data = train,
  control = rpart.control(maxdepth = 5, cp = 0.01)  # Parámetros óptimos
)

# 3. Naive Bayes para Clasificación (Entrega 3)
CnaiveBayes <- naiveBayes(
  precio_categoria ~ .-SalePrice,
  data = train
)

# 4. KNN para Clasificación (Entrega 4)
set.seed(123)
CKNN <- train(
  precio_categoria ~ .-SalePrice,
  data = train,
  method = "knn",
  tuneGrid = expand.grid(k = 9),  # Mejor k según entrega 4
  preProcess = c("center", "scale")
)

# 5. Random Forest (Entregas 2 y 7) - Mejor modelo según entregas
set.seed(123)
Crf <- train(
  precio_categoria ~ .-SalePrice,
  data = train,
  method = "rf",
  ntree = 500,
  importance = TRUE
)


```





# git: <https://github.com/JusSolo/Mineria_Proyecto2.git>

# Introducción:

A lo largo del semestre hemos usado diferentes modelos de aprendizaje supervisado tanto en su version de clasificación como de regresión. Para este último informe del proyecto 2 se pretende probar máquina de vectores de soporte con diferentes topológias para clasificar los precios de las casas en categorías y otras redes para predecir el precio de las mismas. Por otro lado se desea comparar todos los modelos anteriores.

# Modelo de Clasificación con redes Neuronales

### Comparacion de primeros 3 modelos con distintos kernels

```{r, K svm con kernel lineal, include=FALSE}
# Modelo SVM con kernel lineal
Cla_svm1 <- svm(precio_categoria ~ .-SalePrice, data = train, kernel = "linear", scale = TRUE)

# Predicciones en el conjunto de prueba
pred_svm_model1 <- predict(Cla_svm1, newdata = test)

# Matriz de confusión
m1 <- confusionMatrix(pred_svm_model1, test$precio_categoria)
```

```{r, K modelo radial, include=FALSE}
# Modelo SVM con kernel radial
Cla_svm2 <- svm(precio_categoria ~ .-SalePrice, data = train, kernel = "radial", scale = TRUE)

# Predicciones en el conjunto de prueba
pred_svm_model2 <- predict(Cla_svm2, newdata = test)

# Matriz de confusión
m2 <- confusionMatrix(pred_svm_model2, test$precio_categoria)

```

```{r, Kmodelo polinomial, include=FALSE}
# Modelo SVM con kernel polinomial
Cla_svm3 <- svm(precio_categoria ~ .-SalePrice, data = train, kernel = "polynomial", degree = 5, scale = TRUE)

# Predicciones en el conjunto de prueba
pred_svm_model3 <- predict(Cla_svm3, newdata = test)

# Matriz de confusión
m3 <- confusionMatrix(pred_svm_model3, test$precio_categoria)

```

```{r, echo=FALSE,KMatrizConfucion}
# --- Paquetes necesarios ---
library(ggplot2)
library(patchwork)

# --- 1) Preparar heatmaps de confusión --- 

# Función mejorada que convierte confusionMatrix a data.frame
cm_to_df <- function(cm, model_name) {
  df <- as.data.frame(cm$table)
  names(df) <- c("Reference", "Prediction", "Freq")
  df$Reference <- factor(df$Reference, levels = rev(levels(df$Reference)))
  df$Model <- model_name  # Añadir columna para identificar el modelo
  df
}

# Extraer los data.frames con nombres de modelo
df1 <- cm_to_df(m1, "Lineal")
df2 <- cm_to_df(m2, "Radial")
df3 <- cm_to_df(m3, "Polinomial")

# Combinar todos los datos para escala consistente
combined_df <- rbind(df1, df2, df3)
max_freq <- max(combined_df$Freq)

# Función de visualización mejorada
plot_cm <- function(df) {
  ggplot(df, aes(x = Prediction, y = Reference, fill = Freq)) +
    geom_tile(color = "white", linewidth = 0.5) +
    geom_text(aes(label = Freq), size = 4, color = "black") +
    scale_fill_gradientn(
      limits = c(0, max_freq),
      colors = c("#f7fbff", "#4292c6", "#08306b"),
      na.value = "white"
    ) +
    facet_wrap(~Model, ncol = 1) +  # Mostrar en vertical
    labs(title = "Matrices de Confusión por Modelo") +
    theme_minimal(base_size = 12) +
    theme(
      plot.title = element_text(hjust = 0.5, face = "bold", size = 14),
      axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1),
      strip.text = element_text(face = "bold", size = 12),
      panel.spacing = unit(1, "lines"),
      legend.position = "right"
    ) +
    coord_fixed() +
    labs(x = "Predicción", y = "Valor Real")
}

# Visualización única con facetas verticales
plot_cm(combined_df)
```
Observando las matrices de confución se puede concluir que los 3 modelos se equivocan más clasificando la categoría de las casas intermedias. El modelo radial y lineal parecen ser los mejores, Siendo el radial más equilibrado y el lineal mejor en la clasificacion entre las casas econimicas e intermedias. 

```{r, echo=FALSE, KMetricasTest}
library(dplyr)
library(knitr)

# función que, dado un objeto confusionMatrix, devuelve un data.frame con métricas agregadas
get_metrics <- function(cm){
  # métricas globales
  acc  <- cm$overall["Accuracy"]
  kap  <- cm$overall["Kappa"]
  # por‑clase
  sens <- cm$byClass[,"Sensitivity"]
  prec <- cm$byClass[,"Pos Pred Value"]
  # promedios macro
  macro_sens  <- mean(sens, na.rm=TRUE)
  macro_prec  <- mean(prec, na.rm=TRUE)
  macro_f1    <- mean(2 * sens * prec / (sens + prec), na.rm=TRUE)
  data.frame(
    Accuracy    = acc,
    Kappa       = kap,
    Sensitivity = macro_sens,
    Precision   = macro_prec,
    F1          = macro_f1
  )
}

# ensamblar tabla
metrics <- rbind(
  Linear     = get_metrics(m1),
  Radial     = get_metrics(m2),
  Polynomial = get_metrics(m3)
)

# mostrar con 3 decimales
kable(metrics, digits = 3, caption = "Comparación de métricas macro‑promediadas para los 3 modelos SVM usando datos de prueba")

```


```{r, echo=FALSE, KmetricasTrain}
# Predicciones sobre entrenamiento
pred_train1 <- predict(Cla_svm1, newdata = train)
pred_train2 <- predict(Cla_svm2, newdata = train)
pred_train3 <- predict(Cla_svm3, newdata = train)

# Métricas de evaluación en train
m_train1 <- confusionMatrix(pred_train1, train$precio_categoria)
m_train2 <- confusionMatrix(pred_train2, train$precio_categoria)
m_train3 <- confusionMatrix(pred_train3, train$precio_categoria)



# ensamblar tabla
metrics <- rbind(
  Linear     = get_metrics(m_train1),
  Radial     = get_metrics(m_train2),
  Polynomial = get_metrics(m_train3)
)

# mostrar con 3 decimales
kable(metrics, digits = 3, caption = "Comparación de métricas macro‑promediadas para los 3 modelos SVM usando datos de entrenamiento")
```




Comparado ambas tablas el único modelo que parece sobre ajustado es el polynomial, pues es que tiene mayores diferencias en las métricas de desempeño con los datos de prueba y entrenamiento. 








## Modelo ajustado

```{r, ajustar svm clustering, include=FALSE}

set.seed(2025)

# 1) Defino control de entrenamiento: 5‑fold CV estratificado
ctrl <- trainControl(
  method      = "cv",
  number      = 5,
  classProbs  = TRUE,
  summaryFunction = multiClassSummary,
  verboseIter = TRUE
)

# 2) Defino la malla de búsqueda para C y sigma (γ)
# caret usa 'sigma' en lugar de γ
grid <- expand.grid(
  C     = 10^seq(-2, 2, length = 5),
  sigma = 10^seq(-3, 1, length = 5)
)

# 3) Ejecuto train() sobre x_train/yC_train
svmRadial_tuned <- train(
  precio_categoria ~ . -SalePrice,  
  data = train,                   
  method = "svmRadial",
  metric = "Accuracy",
  trControl = ctrl,
  tuneGrid = grid,
  preProc = c("center", "scale")
)
# 4) Resultados
print(svmRadial_tuned)

# 5) Mejor combinación
best <- svmRadial_tuned$bestTune
cat("Mejores hiperparámetros:\n")
print(best)

```


Entre los 3 modelos anteriores se decidió ajustar el modelo radial, pues es más flexible que el lineal y fue un poco pero que el lineal. Posiblemente al ajustarlo su desempeño mejore. 


```{r, mejores hiperparametros 2, echo=FALSE}

best <- svmRadial_tuned$bestTune
cat("Mejores hiperparámetros:\n")
print(best)

```


```{r, echo=FALSE}
# Predicciones con el modelo ajustado
pred_train_tuned <- predict(svmRadial_tuned, newdata = train)
pred_test_tuned  <- predict(svmRadial_tuned, newdata = test)

# Matrices de confusión
m_train_tuned <- confusionMatrix(pred_train_tuned, train$precio_categoria)
m_test_tuned  <- confusionMatrix(pred_test_tuned, test$precio_categoria)


# Usar la función definida previamente
metrics_tuned <- rbind(
  "Tuned Radial (Train)" = get_metrics(m_train_tuned),
  "Tuned Radial (Test)"  = get_metrics(m_test_tuned)
)

# Mostrar en tabla
kable(metrics_tuned, digits = 3, caption = "Métricas macro‑promediadas del modelo SVM Radial tuneado (entrenamiento vs prueba)")

```
El modelo tuneado tiene claramente sobre ajuste y es peor, por lo que vamos a volver a ujustarlo pero separando los datos de entrenamiento en 2 entrenamiento y validación para evitar sobre ajuste.
```{r, include=FALSE}
# Crear partición: 80% para entrenamiento, 20% para validación
set.seed(123)
trainIndex <- createDataPartition(train$precio_categoria, p = 0.8, list = FALSE)
subtrain <- train[trainIndex, ]
valid <- train[-trainIndex, ]

# Definir grid de hiperparámetros
tune_grid <- expand.grid(
  sigma = c(0.001, 0.01, 0.05),  # Parámetro del kernel radial
  C = c(1, 10, 100)               # Parámetro de costo
)

# Control de entrenamiento con validación cruzada interna
ctrl <- trainControl(
  method = "cv", 
  number = 5,
  classProbs = TRUE,              # Necesario para classification
  summaryFunction = multiClassSummary  # Para métricas multiclase
)

# Ajustar modelo SVM Radial
set.seed(123)
svmRadial_tuned_cv <- train(
  precio_categoria ~ . -SalePrice,  # Fórmula que excluye SalePrice
  data = subtrain,
  method = "svmRadial",
  tuneGrid = tune_grid,
  trControl = ctrl,
  preProcess = c("center", "scale"),
  metric = "Accuracy"               # Métrica a optimizar
)


```

```{r, mejores hiperparametros, echo=FALSE}

best <- svmRadial_tuned_cv$bestTune
cat("Mejores hiperparámetros:\n")
print(best)

```
```{r, echo=FALSE}
pred_valid_cv <- predict(svmRadial_tuned_cv, newdata = train)
pred_test_cv  <- predict(svmRadial_tuned_cv, newdata = test)

# Matrices de confusión ya obtenidas
m_valid_cv <- confusionMatrix(pred_valid_cv, train$precio_categoria)
m_test_cv  <- confusionMatrix(pred_test_cv, test$precio_categoria)

# Armar tabla con métricas macro-promediadas
metrics_cv <- rbind(
  "Validación (subtrain)" = get_metrics(m_valid_cv),
  "Prueba (test final)"   = get_metrics(m_test_cv)
)

# Mostrar tabla con formato limpio
kable(metrics_cv, digits = 3, caption = "Métricas del modelo SVM Radial tuneado (con train)")


```

```{r, echo=FALSE}
# Extraer la tabla de confusión como data frame
conf_df <- as.data.frame(m_test_cv$table)

# Graficar como heatmap
ggplot(conf_df, aes(x = Prediction, y = Reference, fill = Freq)) +
  geom_tile(color = "white") +
  geom_text(aes(label = Freq), color = "black", size = 5) +
  scale_fill_gradient(low = "white", high = "steelblue") +
  labs(
    title = "Matriz de Confusión - Modelo SVM Radial (Test)",
    x = "Predicción",
    y = "Valor Real"
  ) +
  theme_minimal()

```

Viendo las métricas y las matrices de confusión y las métricas el modelo tuneado radial parece ser un poco mejor que el lineal, pero por muy poco. 



# Comparacion de todos los modelos de clasificacion usados a lo largo del proyecto

### Matrices de confución

```{r, echo=FALSE}
# Calcular cada matriz de confusión por separado
conf_matrix_rlog <- confusionMatrix(
  predict(CLogistic, newdata = test),
  test$precio_categoria
)

preds_arbol <- predict(Carbol, newdata = test, type = "class")
conf_matrix_arbol <- confusionMatrix(factor(preds_arbol, levels = levels(test$precio_categoria)), 
                                    test$precio_categoria)



conf_matrix_nb <- confusionMatrix(
  predict(CnaiveBayes, newdata = test),
  test$precio_categoria
)

conf_matrix_knn <- confusionMatrix(
  predict(CKNN, newdata = test),
  test$precio_categoria
)

conf_matrix_rf <- confusionMatrix(
  predict(Crf, newdata = test),
  test$precio_categoria
)

conf_matrix_svm <- confusionMatrix(
  predict(svmRadial_tuned_cv, newdata = test),
  test$precio_categoria
)

# Convertir cada matriz a data.frame
df_rlog <- cm_to_df(conf_matrix_rlog, "Logística")
df_arbol <- cm_to_df(conf_matrix_arbol, "Árbol")
df_nb <- cm_to_df(conf_matrix_nb, "N Bayes")
df_knn <- cm_to_df(conf_matrix_knn, "KNN")
df_rf <- cm_to_df(conf_matrix_rf, "R Forest")
df_svm <- cm_to_df(conf_matrix_svm, "SVM Rad")

# Combinar todos los data.frames
combined_df1 <- rbind(df_rlog, df_arbol, df_nb)
combined_df2 <- rbind( df_knn, df_rf, df_svm)

# Visualización (asumiendo que plot_cm está definida)
plot_cm(combined_df1)
plot_cm(combined_df2)
```
Observado las matrices de confucion se puede notar que naive bayes el el único modelo que no clasifica para nada bien. Los demás parecen mas o menos igual de buenos clasificando, observar las matrices de confucion es un poco engorroso. 

### Tablas comparando Espacio en memoría usada en ejecucion y tiempo de ejecucion

```{r, include=FALSE}
accuracy_arbol <- conf_matrix_arbol$overall["Accuracy"]
```


```{r, compara tiempo de ejecucion memoria usada y precicion, echo=FALSE}
#' Perfilar modelos de clasificación
#'
#' @param modelos Lista de modelos a evaluar
#' @param datos Datos de test para evaluación
#' @param target Nombre de la variable objetivo
#' @return Dataframe con métricas de performance
profile_modelos <- function(modelos, datos, target) {
  library(pryr)
  library(microbenchmark)
  library(dplyr)
  
  resultados <- data.frame()
  
  for (nombre in names(modelos)) {
    modelo <- modelos[[nombre]]
    
    # Medir tiempo de predicción
    tiempo <- microbenchmark(
      predict(modelo, newdata = datos),
      times = 10,
      unit = "ms"
    ) %>% summary() %>% pull(median)
    
    # Medir uso de memoria
    mem_size <- object_size(modelo) %>% as.numeric()
    
    # Calcular precisión
    preds <- predict(modelo, newdata = datos)
    acc <- mean(preds == datos[[target]])
    
    # Almacenar resultados
    resultados <- rbind(resultados, data.frame(
      Modelo = nombre,
      Tiempo_prediccion_ms = tiempo,
      Memoria_MB = round(mem_size/1024/1024, 2),
      Accuracy = round(acc, 4)
    ))
  }
  
  return(resultados)
}

# Preparar los modelos en una lista nombrada
modelos_a_comparar <- list(
  "Regresión Logística" = CLogistic,
  "Árbol de Decisión" = Carbol,
  "Naive Bayes" = CnaiveBayes,
  "KNN" = CKNN,
  "Random Forest" = Crf,
  "SVM Radial" = svmRadial_tuned_cv
)

# Ejecutar el profiling
resultados_comparacion <- profile_modelos(
  modelos = modelos_a_comparar,
  datos = test,
  target = "precio_categoria"
)

resultados_comparacion$Accuracy[2] <- accuracy_arbol

# Mostrar resultados ordenados por tiempo
resultados_comparacion %>% 
  arrange(Tiempo_prediccion_ms) %>%
  knitr::kable(
    caption = "Comparación de complejidad espacial y temporal de modelos ordenados por tiempo de ejecucion",
    col.names = c("Modelo", "Tiempo (ms)", "Memoria (MB)", "Precisión"),
    digits = 3
  )

# Mostrar resultados ordenados por memoria
resultados_comparacion %>% 
  arrange(Memoria_MB) %>%
  knitr::kable(
    caption = "Comparación de complejidad espacial y temporal de modelos ordenados por espacio en memoria",
    col.names = c("Modelo", "Tiempo (ms)", "Memoria (MB)", "Precisión"),
    digits = 3
  )



```
```{r, echo=FALSE}

resultados_comparacion %>%
  arrange(desc(Accuracy)) %>%
  knitr::kable(
    caption = "Comparación de modelos ordenados por precisión",
    col.names = c("Modelo", "Tiempo (ms)", "Memoria (MB)", "Precisión"),
    digits = 3
  )

```

Arriba se puede observar la misma tabla pero ordenada según cada una de sus columnas. Como el SVN es el único modelo que aparece en el top 3 en las 3 tablas podemos considerar que en términos generales es el mejor modelo. Si se toma en cuenta el tiempo de ejecución y la presicion el mejor modelo es la regresión logística, pero es el segundo que ocupa más memoria. Para estos datos de la manera que fueron tratados el peor modelo es Naive Bayes, esto puede deberse a que algunas de las variables no son independientes (ir a ver nalisis exploratorio informe 1). 
  En conclucion exceptuando Naive Bayes todos los modelos funcionan. Si se necesitara un modelo muy rapido y que no ocupe mucha memorio (en la ejecucion) el algoritmo a elegir sería El Árbo de desicion. Si se buscara la presicion a toda costa seria el Random Forest el elegido. Pero si se desea un eqilibrio entre los 3 el modelo a elegir sería le regresión logística (si no importa mucho el espacio en memoría) y un SVM radial (si no importa que el tiempo de ejecucion sea mas lento).
  En conclusión no hay un modelo que sea el mejor en todo. El que se elija usar debería ser el que se pueda ejecutar en el tiempo y espeacio que se requiere con la presicion más alta. Para tomar esa desicion podria ser util usar alguna funcion de costo que pondere las metricas y elegir el modelo que lo minimice y cumpla con las restricciones. 
  
# Modelo de Regresion con redes Neuronales

Para los modelos de regresión, se seleccionó la variable (SalePrice) como variable respuesta, ya que es el que representa el precio de venta de las casas y es el objetivo principal.

Se entrenan dos modelos de redes neuronales con distintas topoloias y parámetros, entrenados con validacion cruzada.
```{r include=FALSE}
library(nnet)

# Modelo 1: Red neuronal simple
set.seed(123)
RNA1 <- train(
  SalePrice ~ . -precio_categoria,
  data = train,
  method = "nnet",
  linout = TRUE,
  trace = FALSE,
  tuneGrid = expand.grid(size = 5, decay = 0.01),
  trControl = trainControl(method = "cv", number = 5)
)

# Modelo 2: Topología diferente
set.seed(123)
RNA2 <- train(
  SalePrice ~ . -precio_categoria,
  data = train,
  method = "nnet",
  linout = TRUE,
  trace = FALSE,
  tuneGrid = expand.grid(size = 10, decay = 0.001),
  trControl = trainControl(method = "cv", number = 5)
)

```


```{r,  echo=FALSE}
# Evaluar RMSE en test
pred1 <- predict(RNA1, newdata = test)
pred2 <- predict(RNA2, newdata = test)

rmse1 <- RMSE(pred1, test$SalePrice)
rmse2 <- RMSE(pred2, test$SalePrice)

data.frame(Modelo = c("RNA1", "RNA2"), RMSE = c(rmse1, rmse2))


```
Al comparar los dos modelos de redes neuronales, se observa que el modelo RNA1 tiene un RMSE de 63260.48 a comparación con RNA2 que tiene un RMSE de 76804.70, indicando que RNA1 es el que mejor rendimiento tiene de estos dos modelos. 

```{r , include=FALSE}
library(nnet)
library(caret)
library(Metrics)

set.seed(123)

# Grid de parámetros
tune_grid <- expand.grid(
  size = c(3, 5, 7, 10),
  decay = c(0.0001, 0.001, 0.01)
)

# Entrenamiento con validación cruzada
RNA_cv <- train(
  SalePrice ~ . -precio_categoria,
  data = train,
  method = "nnet",
  linout = TRUE,
  trace = FALSE,
  tuneGrid = tune_grid,
  trControl = trainControl(method = "cv", number = 5)
)

```


La verificacion de sobreajusto se realizó una gráfica de curva de aprendizaje. En el que no hay sobreajuste. 


```{r,  echo=FALSE}
# Mostrar combinación óptima de parámetros
RNA_cv$bestTune

# Crear tabla de resultados
resultados <- data.frame()

for (i in 1:nrow(tune_grid)) {
  params <- tune_grid[i, ]

  modelo <- nnet(
    SalePrice ~ . -precio_categoria,
    data = train,
    linout = TRUE,
    size = params$size,
    decay = params$decay,
    maxit = 500,
    trace = FALSE
  )

  # Predicciones en training y test
  pred_train <- predict(modelo, train)
  pred_test <- predict(modelo, test)

  # Calcular errores
  rmse_train <- RMSE(pred_train, train$SalePrice)
  rmse_test <- RMSE(pred_test, test$SalePrice)

  # Guardar resultados
  resultados <- rbind(resultados, data.frame(
    size = params$size,
    decay = params$decay,
    RMSE_Train = rmse_train,
    RMSE_Test = rmse_test
  ))
}

print(resultados)


```



```{r,  echo=FALSE}
library(ggplot2)

ggplot(resultados, aes(x = interaction(size, decay))) +
  geom_line(aes(y = RMSE_Train, group = 1, color = "Train")) +
  geom_line(aes(y = RMSE_Test, group = 1, color = "Test")) +
  labs(
    x = "Topología (size-decay)", y = "RMSE",
    title = "Curva de aprendizaje - RNA",
    color = "Dataset"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```
Se realizó sobreajuste al modelo RNA utilizando la validacion cruzada, mostrando que la mejor combinacion fue: el de tamaño 5 y decay 0.1


```{r,  echo=FALSE}
grid <- expand.grid(size = c(5, 10, 15), decay = c(0.001, 0.01, 0.1))

set.seed(123)
RNA_tuned <- train(
  SalePrice ~ . -precio_categoria,
  data = train,
  method = "nnet",
  linout = TRUE,
  trace = FALSE,
  tuneGrid = grid,
  trControl = trainControl(method = "cv", number = 5)
)

# Ver mejor combinación
RNA_tuned$bestTune

```

Este modelo final mejoró levemente el RMSE respecto a los modelos iniciales, sin sobreajuste, ya que los errores se mantuvieron cercanos, tanto para entrenamiento como validación. 


# Comparacion de todos los modelos de regresion usados a lo largo del proyecto.

Se comparó el rendimiento del mejor modelo de RNA con modelos realizados anteriormente, como regresión lineal, KNN y arbol, Naive Bayes. Se muestrar a continuacion los resultados.



```{r, echo=FALSE}
resumen <- data.frame(
  Modelo = c("Regresión Lineal", "Árbol", "KNN", "Naive Bayes", "RNA1", "RNA2", "RNA Tuned"),
  RMSE = c(
    RMSE(predict(Rlineal, test), test$SalePrice),
    RMSE(predict(Rarbol, test), test$SalePrice),
    RMSE(predict(RKNN, test), test$SalePrice),
    NA,  # No aplica Naive Bayes para regresión directa
    rmse1,
    rmse2,
    RMSE(predict(RNA_tuned, test), test$SalePrice)
  )
)
print(resumen)

```
Entre todos los modelos de regresion entrenados, el mejor modelo fue de Regresion lineal ya que tiene menos error con un RMSE de 36745.35 secundado por KNN DE 39930.13. Notese que ninguno de los modelos de RNA supero a la regresion lineal 

```{r, echo=FALSE}
RNA_clas <- train(
  precio_categoria ~ . -SalePrice,
  data = train,
  method = "nnet",
  trace = FALSE,
  tuneGrid = expand.grid(size = 5, decay = 0.01),
  trControl = trainControl(method = "cv", number = 5)
)

pred_class <- predict(RNA_clas, newdata = test)
confusionMatrix(pred_class, test$precio_categoria)

```
```{r, echo=FALSE}
library(randomForest)

set.seed(123)
Rforest <- train(
  precio_categoria ~ . -SalePrice,
  data = train,
  method = "rf",
  trControl = trainControl(method = "cv", number = 5)
)


pred_rf <- predict(Rforest, newdata = test)
confusionMatrix(pred_rf, test$precio_categoria)

```


```{r, echo=FALSE}



acc_rna <- confusionMatrix(pred_class, test$precio_categoria)$overall["Accuracy"]
acc_rf <- confusionMatrix(predict(Rforest, test), test$precio_categoria)$overall["Accuracy"]
acc_knn <- confusionMatrix(predict(CKNN, test), test$precio_categoria)$overall["Accuracy"]

accuracy_table <- data.frame(
  Modelo = c("RNA", "Random Forest", "KNN"),
  Accuracy = c(acc_rna, acc_rf, acc_knn)
)
print(accuracy_table)

```
En clasificación, el modelo Random Forest obtuvo mejor precisión, por el Acuaracy (0.8394495) segudio por KNN, el modelo RNA logro un buen desempeño, aunque no el mejor,   


```{r, echo=FALSE}
system.time(predict(Rlineal, newdata = test))
system.time(predict(RNA_tuned, newdata = test))

```
Dada que las redes neuronales no fueron muy precisas  en regresión, el modelo tuneado obtuvo mejor significativa respecto a RNA iniciales. Pero la regresión lineal sigue siendo el modelo mas preciso.

# Conclusiones
El mejor modelo de regresion fue la regesion lineal, superando al modelo RNA tuneado.
En clasificación, random Forest fue el mas preciso. 
Las redes neuronales fueron útiles, pero no superaron a los medelos mas simples
Los modelos de regresión lineal   y Random Forest son los recomendados para este conjunto de datos.













