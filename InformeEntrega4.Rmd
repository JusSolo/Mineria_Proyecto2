---
title: "Informe Proyecto 2 entrega 4"
author:
- "Juan Luis Solórzano (carnet: 201598)"
- "Micaela Yataz (carnet: 18960)"
date: "2025-01-20"
output: pdf_document
---



```{r, include=FALSE}
# Cargar librerías necesarias
library(randomForest)
library(dplyr)
library(GGally)
library(ggplot2)
library(tidyr)
library(caret)
library(rpart)
library(rpart.plot)
library(glmnet)
library(e1071)
library(Metrics)
library(tidyverse)
library(class)



# Cargar la base de datos
datos <- read.csv('train.csv')

# Definir variables cuantitativas
vars_cuantitativas <- c("SalePrice", "LotFrontage", "LotArea", "OverallQual", "OverallCond", 
                        "YearBuilt", "YearRemodAdd", "MasVnrArea", "BsmtFinSF1", "BsmtFinSF2", 
                        "BsmtUnfSF", "TotalBsmtSF", "X1stFlrSF", "X2ndFlrSF", "LowQualFinSF", 
                        "GrLivArea", "BsmtFullBath", "BsmtHalfBath", "FullBath", "HalfBath", 
                        "BedroomAbvGr", "KitchenAbvGr", "TotRmsAbvGrd", "Fireplaces", "GarageYrBlt", 
                        "GarageCars", "GarageArea", "WoodDeckSF", "OpenPorchSF", "EnclosedPorch", 
                        "X3SsnPorch", "ScreenPorch", "PoolArea", "MiscVal", "MoSold", "YrSold")

# Imputar valores faltantes en variables específicas
datos <- datos %>%
  mutate(
    LotFrontage = ifelse(is.na(LotFrontage), 0, LotFrontage),
    MasVnrArea = ifelse(is.na(MasVnrArea), 0, MasVnrArea),
    GarageYrBlt = ifelse(is.na(GarageYrBlt), median(GarageYrBlt, na.rm = TRUE), GarageYrBlt)
  )

# Seleccionar solo variables cuantitativas
datosC <- datos[, vars_cuantitativas]
```




# git: https://github.com/JusSolo/Mineria_Proyecto2.git


# 1. Elabore un modelo de regresión usando K nearest Neighbors (KNN), el conjunto de entrenamiento y la variable respuesta SalesPrice. Prediga con el modelo y explique los resultados a los que llega. Asegúrese que los conjuntos de entrenamiento y prueba sean los mismos de las entregas anteriores para que los modelos sean comparables.

```{r, regresion usando KNN}
y<- datos$SalePrice
set.seed(123)
trainI<- createDataPartition(y, p=0.7, list=FALSE)

train<-datosC[trainI, ]
test<-datosC[-trainI, ]

## Modelo:
train<-train[complete.cases(train),]
k_vecinos <- round(sqrt(nrow(datosC)),0)
parametros <- expand.grid(k = k_vecinos)

modelo_knn1 <- train(
  SalePrice~., 
  data=train, 
  method = "knn", 
  preProcess= c("center","scale","knnImpute"),
  tuneGrid = parametros
)
predModelo1 <- predict(modelo_knn1, newdata=test)


```


# 2. Analice los resultados del modelo de regresión usando KNN. ¿Qué tan bien le fue prediciendo? Utilice las métricas correctas.

```{r}

pred_test <- predict(modelo_knn1, newdata = test)

metrics <- postResample(pred_test, test$SalePrice)
print(metrics)

```
 El $R^2= 0.784$ es aceptable, El MAE es del orden de los $20,000 lo que para el precio de una casa parece aceptable. 


# 3. Compare los resultados con el modelo de regresión lineal, el mejor modelo de árbol de regresión y de naive bayes que hizo en las entregas pasadas. ¿Cuál funcionó mejor?

```{r, modelos anteriores, echo=FALSE}
modelo_lineal <- train(
  SalePrice ~ ., 
  data = train, 
  method = "lm",  # Método para regresión lineal
  preProcess = c("center", "scale", "knnImpute")  # Preprocesamiento igual al de KNN
)


arbol <- rpart(SalePrice~.,data = train)


modelo_naivbayes <- naiveBayes(SalePrice ~ ., data = train)

# Predicciones en test para cada modelo
pred_knn <- predict(modelo_knn1, newdata = test)
pred_arbol <- predict(arbol, newdata = test)
pred_lineal <- predict(modelo_lineal, newdata = test)
pred_naivbayes <- as.numeric(predict(modelo_naivbayes, newdata = test))

# Función para calcular métricas con caret
calcular_metricas <- function(y_real, y_pred) {
  resultados <- postResample(y_pred, y_real)  # Devuelve RMSE, R², MAE
  data.frame(
    RMSE = resultados["RMSE"],
    MAE = resultados["MAE"],
    R2 = resultados["Rsquared"],
    r = cor(y_real, y_pred)  # Coeficiente de correlación
  )
}

# Crear tabla comparativa
resultados <- bind_rows(
  KNN = calcular_metricas(test$SalePrice, pred_knn),
  Lineal = calcular_metricas(test$SalePrice, pred_lineal),
  Árbol = calcular_metricas(test$SalePrice, pred_arbol),
  NaiveBayes = calcular_metricas(test$SalePrice, pred_naivbayes),
  .id = "Modelo"
)

# Mostrar tabla
print(resultados)

```

Podemos notar que para todas las medidas de error excepto el MAE el mejor modelo es el KNN, el segundo mejor es el modelo lineal, seguido por el Arbol y el peor es el Naïve Bayes. 

# 4. Haga un modelo de clasificación, use la variable categórica que hizo con el precio de las casas (barata, media y cara) como variable respuesta.

```{r Recuperacion de varibale}
train$precio_categoria<-cut(train$SalePrice, 
                            breaks = c(0, 129975, 214000,Inf),
                            labels = c("Economica", "Intermedia", "Cara" ),
                            include.lowest = TRUE)

test$precio_categoria<-cut(test$SalePrice, 
                            breaks = c(0, 129975, 214000, Inf),
                            labels = c("Economica", "Intermedia", "Cara"),
                            include.lowest = TRUE)
#normalizar datos

x_train<-train[, !(names(train) %in% c("SalePrice", "precio_categoria"))]
y_train<-train$precio_categoria

x_test<- test[, !(names(test) %in% c("SalePrice", "precio_categoria"))]
y_test<-test$precio_categoria


#modelo de casificacion 

modelo_knn_class<-train(
  x=x_train,
  y=y_train,
  method="knn"
  )

```


# 5. Utilice los modelos con el conjunto de prueba y determine la eficiencia del algoritmo para predecir y clasificar.

```{r prediccion}
predic_knn_clas<-predict(modelo_knn_class, newdata = x_test)

```

# 6. Haga un análisis de la eficiencia del modelo de clasificación usando una matriz de confusión. Tenga en cuenta la efectividad, donde el algoritmo se equivocó más, donde se equivocó menos y la importancia que tienen los errores.

```{r matriz de confusion}

consfusion_m<- confusionMatrix(predic_knn_clas, y_test)
print(consfusion_m)

```
El modelo clasificó corectamente el 75.23% de las casas, segun el Accuaracy, es moderado. Hay desbalance de clases, en especifico para la clase Lujo, nótese por la sensibilidad, ya que no identifico a ninguna casa de lujo. La clasificacion erronea de viviendas de lujo como econommicas tienen impacto significativo ya que tienen una gran diferencia en el valor de las propiedades.


# 7.  Analice el modelo. ¿Cree que pueda estar sobreajustado?

```{r}
# analicemos el rendimiento del modelo con los datos de entrenamiento
predic_knn_clas_train<-predict(modelo_knn_class, newdata = x_train)

consfusion_m<- confusionMatrix(predic_knn_clas_train, y_train)
print(consfusion_m)

```
Al hacer una matriz de confucion con los datos de entrenamiento, podelmos notar que clasificó correctamente el 78.91% de las casa, muy similar al rendimiento con los datos de prueba (75.23%) . En general no parece sobre ajustado. Comparemos el rendimiento por clases:



```{r, echo=FALSE}
library(knitr)
library(kableExtra)
library(dplyr)

# Definir los datos
metrics <- c("Sensitivity", "Specificity", "Pos Pred Value", "Neg Pred Value", 
             "Prevalence", "Detection Rate", "Detection Prevalence", "Balanced Accuracy")

data <- data.frame(
  Metric = metrics,
  `Economica (Train)` = c(0.6367, 0.9674, 0.8670, 0.8888, 0.2500, 0.1592, 0.1836, 0.8021),
  `Economica (Test)` = c(0.6239, 0.9694, 0.8718, 0.8855, 0.2500, 0.1560, 0.1789, 0.7966),
  `Intermedia (Train)` = c(0.8755, 0.7176, 0.7576, 0.8512, 0.5020, 0.4395, 0.5801, 0.7966),
  `Intermedia (Test)` = c(0.8539, 0.6498, 0.7110, 0.8150, 0.5023, 0.4289, 0.6032, 0.7518),
  `Cara (Train)` = c(0.7677, 0.9390, 0.8058, 0.9246, 0.2480, 0.1904, 0.2363, 0.8533),
  `Cara (Test)` = c(0.6759, 0.9329, 0.7684, 0.8974, 0.2477, 0.1674, 0.2179, 0.8044)
)

# Mostrar la tabla en formato LaTeX con ajuste de tamaño
kable(data, format = "latex", caption = "Métricas de Test y Train Comparadas") %>%
  kable_styling(latex_options = c("striped", "hold_position", "scale_down"))
```

En la tabla podemos notar que las columnas de test son similares a las de train, por lo que no pare que haya sobre ajuste.

# 8. Haga un modelo usando validación cruzada, compare los resultados de este con los del modelo anterior. ¿Cuál funcionó mejor?

```{r}

trctrl <- trainControl(method = "repeatedcv",
                           number = 10,
                           repeats = 3)
modelo_knn_class_cruss <- train(
  x= x_train, 
  y=y_train, 
  method = "knn", 
  trControl = trctrl, 
  preProcess= c("center","scale")
)

pred_test_cruss<-predict(modelo_knn_class_cruss, newdata = x_test)

consfusion_m<- confusionMatrix(pred_test_cruss, y_test)
print(consfusion_m)


```

Parece que con validación cruzada el modelo es un poco mejor. porque clasifica muy bien las casas económicas pero un poco peor las demás classes. En general la diferencia no parece significativas. 

#9. Tanto para los modelos de regresión como de clasificación, pruebe con varios valores de los hiperparámetros ¿Qué parámetros pueden tunearse en un KNN?, use el mejor modelo del tuneo, ¿Mejoraron los resultados usando el mejor modelo ahora? Explique
```{r}
trctrl <- trainControl(method = "repeatedcv",
                           number = 10,
                           repeats = 3)
# Ajuste de k con valores específicos: Clasificacion Por si te sirve
tune_grid <- expand.grid(k = c(3, 5, 7, 9, 11))
modelo_knn_class <- train(
  x = x_train,
  y = y_train,
  method = "knn",
  trControl = trctrl,
  tuneGrid = tune_grid
)

```


#10. Compare la eficiencia del algoritmo con el resultado obtenido con el árbol de decisión (el de clasificación), el modelo de random forest y el de naive bayes que hizo en las entregas pasadas. ¿Cuál es mejor para predecir? ¿Cuál se demoró más en procesar?

























